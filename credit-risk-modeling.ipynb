{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12699,"sourceType":"datasetVersion","datasetId":9109}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#F2A997;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <h3 style=\"padding: 8px;color:white\"><b>1 | Introduction </b></h3>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 18px; font-weight: bold;\">\n    Credit risk, however, is a significant risk that financial institutions must manage. Because of how significant the risk is, capital allocation could be less expensive and more effective without it. Risk associated with credit is the possibility that a borrower won't be able to pay back their loan. Put otherwise, when a borrower defaults, they don't repay their obligation, which costs financial institutions money.\n</p>\n\n<p style=\"font-size: 18px; font-weight: bold;\">\n    A more explicit definition of credit risk and its objective can be found in BCBS and BIS 2000:\n</p>\n\n<p style=\"font-size: 18px; font-weight: bold;\">\n    The simplest way to define credit risk is the possibility that a counterparty or bank borrower won't fulfill its responsibilities according to the terms set forth.\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 18px; font-weight: bold;\">\n    By keeping credit risk exposure within reasonable bounds, credit risk management seeks to maximize a bank's risk-adjusted rate of return.\n    Because estimating credit risk is such a difficult undertaking, Basel, a regulatory agency, keeps a close eye on current changes in the financial markets and establishes rules to raise the capital requirements for banks. The premise that banks should have a capital buffer in volatile times underpins the significance of having sufficient capital requirements for a bank.\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 18px; font-weight: bold;\">\n    Apart from the probability of default, which indicates the possibility that a borrower won't be able to pay back their debt, credit risk is distinguished by three factors:\n</p>\n<p style=\"font-size: 18px; font-weight: bold; margin-left: 20px;\">\n    Exposure<br>\n    This is a reference to a party that might experience default or a negative shift in its performance capacity.\n</p>\n<p style=\"font-size: 18px; font-weight: bold; margin-left: 20px;\">\n    Likelihood<br>\n    The probability that this party won't fulfill their responsibilities.\n</p>\n<p style=\"font-size: 18px; font-weight: bold; margin-left: 20px;\">\n    Rate of recovery<br>\n    How much is recoverable in the event of a default.\n</p>\n<p style=\"font-size: 18px; font-weight: bold;\">\n    The Basel Accord, often known as the worldwide financial credit management rules, was proposed by the BCBS. As of right now, three Basel Accords exist. The most notable regulation introduced by Basel I in 1988 was the need to maintain capital equal to or more than 8% of assets weighted by risk.\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 18px; font-weight: bold;\">\n    0% for assets devoid of risk; 20% for bank loans; 50% for home mortgages; and 100% for corporate debt.\n    Basel I was revised in 1999 by Basel II, which was founded on three primary pillars:\n</p>\n<ul style=\"font-size: 18px; font-weight: bold; margin-left: 20px;\">\n    <li>Effective use of disclosure as a lever to strengthen market discipline and encourage sound banking practices;</li>\n    <li>Supervisory review of an institution's capital adequacy and internal assessment process;</li>\n    <li>Minimum capital requirements, which aimed to develop and expand the standardized rules set out in the 1988 Accord.</li>\n</ul>\n<p style=\"font-size: 18px; font-weight: bold;\">\n    The 2007–2008 mortgage crisis intensified, making the 2010 Basel III deal necessary. In order to further boost liquidity and weaken governance practices, it introduced a new set of initiatives.\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 18px; font-weight: bold;\">\n    IRB approach; the key parameters of this internal assessment are:\n</p>\n<p><span style=\"background-color: #F2EB97; font-weight: bold;\">Expected loss = EAD × LGD × PD</span></p>\n\n</p>\n<p style=\"font-size: 18px; font-weight: bold; margin-left: 20px;\">\n    where PD is the probability of default, LGD is the expected loss given default taking a value between 0 and 1, and EAD is the exposure at default.\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 18px; font-weight: bold;\">\n    We may develop risk bucketing for credit risk analysis using the K-means algorithm. K-means uses the cluster center, or centroid, as a basis for calculating the distance between observations inside the cluster. The clustering of observations is dependent on the distance to the centroid. There are several ways to measure this distance.\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"$$\n\\text{Euclidean} = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}\n$$\n\n\n$$\n\\text{Minkowski} = \\left(\\sum_{i=1}^{n} |p_i - q_i|^p \\right)^{1/p}\n$$\n\n\n$$\n\\text{Manhattan} = \\sqrt{\\sum_{i=1}^{n} |p_i - q_i|}\n$$\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 18px; font-weight: bold;\">\n    Reducing the distance between the centroid and the observations is the goal of clustering, as this will ensure that similar observations are in the same cluster. This reasoning is based on the intuition that observations are closer to one another the more similar they are. Therefore, our goal is to minimize the sum of the squared errors between the centroid and the observations, or alternatively, the distance between the observations and the centroid:\n\n$$\n\\sum_{i=1}^{K} \\sum_{x \\in C_i} (C_i - x)^2\n$$</p>\n<p style=\"font-size: 14px; font-weight: bold;\">\nwhere <em>x</em> is an observation and <em>C<sub>i</sub></em> is the centroid of the <em>i</em>th cluster.</p>\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 18px; font-weight: bold;\">The E-M algorithm minimizes distances iteratively between centroids and observations. The algorithm functions as follows:</p>\n\n<ol style=\"font-size: 15px; font-weight: bold;\">\n    <li>Select k sites at random to serve as centroids.</li>\n    <li>Determine the distances between observations and the n centroids using the selected distance metric. Assign each observation to the nearest cluster based on these distances.</li>\n    <li>Adjust cluster centers according to the allocation.</li>\n    <li>Repeat from step 2 until centroids remain unchanged.</li>\n</ol>\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 18px; font-weight: bold;\">The total of the squared distances between each observation and its nearest centroid is used to calculate inertia. Second, a method for determining the ideal number of clusters is presented: the Silhouette score. It accepts a value in the range of 1 and -1. When an observation has a value of 1, it has been accurately classified and is in close proximity to the correct centroid. On the other hand, -1 indicates that an observation is not properly clustered. Considering both the intracluster and intercluster distances is what gives the Silhouette score its strength. The following is the formula for Silhouette score:</p>\n\n<p style=\"font-size: 18px; font-weight: bold;\">\n    \\[\n    \\text{Silhouette score} = \\frac{x - y}{\\max(x, y)}\n    \\]\n</p>\n<ol style=\"font-size: 15px; font-weight: bold;\">where x is the average intercluster distance between clusters, and y is the mean intracluster\ndistance.","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 18px; font-weight: bold;\">\n    <strong>Age:</strong> Numerical\n</p>\n\n<p>The data includes the following:</p>\n<ul>\n    <li><strong>Gender:</strong> male or female</li>\n    <li><strong>Job:</strong> \n        <ul>\n            <li>0 — unskilled and non-resident</li>\n            <li>1 — unskilled and resident</li>\n            <li>2 — skilled</li>\n            <li>3 — highly skilled</li>\n        </ul>\n    </li>\n    <li><strong>Housing:</strong> own, rent, free</li>\n    <li><strong>Savings accounts:</strong> little, moderate, pretty rich, rich</li>\n</ul>\n\n<p>The following are numerical:</p>\n<ul>\n    <li><strong>Checking account\n    <li><strong>Credit amount\n    <li><strong>Duration\n    <li><strong>Purpose: vehicle, furnishings/equipment, TV/radio, home appliances, maintenance, business, travel, and other purposes</li>\n    <li><strong>Risk:</strong> good (1), bad (2)</li>\n</ul>\n","metadata":{}},{"cell_type":"markdown","source":"# In this notebook I modeling data of credit risk for Probability of Default Estimation with` Kmean,SVC ,Decision Tree Classifier ,RandomForestClassifier ,GaussianNB and NN`","metadata":{}},{"cell_type":"code","source":"#pip install git+git://github.com/milesgranger/gap_statistic.git","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-10-11T17:39:34.481188Z","iopub.execute_input":"2024-10-11T17:39:34.481586Z","iopub.status.idle":"2024-10-11T17:39:34.518636Z","shell.execute_reply.started":"2024-10-11T17:39:34.481552Z","shell.execute_reply":"2024-10-11T17:39:34.517428Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade gap-stat","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-10-11T17:39:34.521213Z","iopub.execute_input":"2024-10-11T17:39:34.521662Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7bea4ae1c100>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/gap-stat/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7bea4ae1c850>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/gap-stat/\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#F2A997;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <h3 style=\"padding: 8px;color:white\"><b>2 | Libraries </b></h3>","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd\nimport missingno as msno \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport warnings\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom imblearn.combine import SMOTEENN\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.cluster import KMeans\nfrom gap_statistic.optimalK import OptimalK\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.metrics import confusion_matrix, silhouette_score\nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom sklearn.preprocessing import StandardScaler\nfrom IPython.display import display, Markdown\n# Import matplotlib for plotting\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.experimental import enable_halving_search_cv  # Needed for HalvingRandomSearchCV\nfrom sklearn.model_selection import HalvingRandomSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.svm import SVC\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingRandomSearchCV\nimport time\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom IPython.display import Markdown, display\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\n\nimport plotly.subplots as tls\nimport plotly.graph_objs as go\nimport plotly.subplots as tls\nimport plotly.offline as py  \nfrom sklearn.metrics import f1_score\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score # to split the data\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, fbeta_score #To evaluate our model\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import model_selection\n# Algorithmns models to be compared\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nimport tensorflow as tf\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.metrics import roc_auc_score\n\n# Import SilhouetteVisualizer from Yellowbrick for silhouette plot visualization\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\n# Define custom color palette\ncustom_palette = ['#F6B4A4', '#F78165', '#F46644', '#DD3F19', '#A7391F', '#97331C']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#F2A997;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <h3 style=\"padding: 8px;color:white\"><b>3 | Import data </b></h3>","metadata":{}},{"cell_type":"code","source":"credit = pd.read_csv('/kaggle/input/german-credit-data-with-risk/german_credit_data.csv')\ncredit\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove column unnamed\ndel credit['Unnamed: 0']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#F2A997;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <h3 style=\"padding: 8px;color:white\"><b>3 | Analysis of data  </b></h3>","metadata":{}},{"cell_type":"code","source":"# checking missing data\ndef get_missing_data(credit):\n    total = credit.isnull().sum().sort_values(ascending = False)\n    percent = (credit.isnull().sum()/credit.isnull().count()*100).sort_values(ascending = False)\n    percent = round(percent,2)\n    missing_df  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    print( missing_df.head(20))\n    return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_missing_data(credit)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.matrix(credit) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #F9D3CA; padding: 15px; border-radius: 5px;\">\n  <p>We have missing value in columns Saving accounts and Checking account .</p>\n  \n","metadata":{}},{"cell_type":"code","source":"# Create a copy of the dataset\ndf_copy = credit.copy()\n\n# Define the mapping for the job column\njob_mapping = {\n    0: 'unskilled and non-resident',\n    1: 'unskilled and resident',\n    2: 'skilled',\n    3: 'highly skilled'\n}\n\n# Convert numerical values to categorical values in the job column\ndf_copy['Job'] = df_copy['Job'].map(job_mapping)\n\n# Display the first few rows of the modified dataset to verify the changes\nprint(df_copy.head())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split categorical and numerical dataset\ncat_cols = ['Sex', 'Housing', 'Saving accounts', 'Checking account','Job', 'Purpose']\nnum_cols = list(set(df_copy.columns) - set(cat_cols))\nprint('='*7, 'Statistical Describtion of Data by Type: int64, float64', '='*7)\ndisplay(df_copy[num_cols].describe().T)\nprint('\\n', '='*7, 'Statistical Describtion of Data by Type: category', '='*7)\ndisplay(df_copy[cat_cols].describe().T)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 3, figsize=(14, 10))\n\nfor i, col_name in enumerate(cat_cols):\n    sns.countplot(data=df_copy, x=col_name, ax=axes[i//3, i%3], palette=custom_palette)\n    axes[i//3, i%3].set_title(f'Value Counts in $\\\\mathbf{{{col_name}}}$')\n    axes[i//3, i%3].set_xticklabels(axes[i//3, i%3].get_xticklabels(), rotation=45, ha='right')\n\n# Remove empty subplots if any\nfor j in range(len(cat_cols), 9):\n    fig.delaxes(axes.flatten()[j])\n\nplt.tight_layout(pad=3)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #F9D3CA; padding: 20px; border-radius: 5px;\">\n    <b>Value counts:</b>\n    <ul>\n        <li><b>Checking accounts:</b> The value counts are higher than those in savings and housing accounts.</li>\n        <li><b>Savings accounts:</b> The value counts are higher than those in housing accounts.</li>\n        <li><b>Housing accounts:</b> The value counts are the lowest.</li>\n        <li><b>Sex:</b> The chart shows a higher value count for males than females.</li>\n        <li><b>Job:</b> The chart shows a higher value count for people with skilled jobs than those with little or no skill.</li>\n        <li><b>Purpose:</b> The chart shows a higher value count for people using their money for radio/TV than other purposes.</li>\n    </ul>\n    <p>Overall, the chart seems to indicate that people have more money in their checking accounts than in their savings and housing accounts. Men appear to have more money than women, and people with skilled jobs tend to have more money than those with little or no skills. The purpose for which people are using their money seems varied, with radio/TV being the most common purpose according to the chart.</p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Set the custom palette for seaborn\nsns.set_palette(custom_palette)\n\nfig, ax = plt.subplots(5, 2, figsize=(12, 20))\n\nfor i, col in enumerate(df_copy.columns):\n    sns.histplot(df_copy[col], kde=True, ax=ax[i//2, i%2], color=custom_palette[i % len(custom_palette)])\n    ax[i//2, i%2].set_title(f'Distribution of {col}')\n    for label in ax[i//2, i%2].get_xticklabels():\n        label.set_rotation(45)\n        label.set_ha('right')\n\n# Remove empty subplot if any\nif len(df_copy.columns) % 2 != 0:\n    fig.delaxes(ax.flatten()[-1])\n\nfig.suptitle('Distribution of Columns', fontsize=16)\nplt.tight_layout(pad=2)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #F9D3CA; padding: 20px; border-radius: 5px; margin-top: 20px;\">\n    <b>Distributions:</b>\n    <ul>\n        <li><b>Distribution of Age:</b> The x-axis represents age, and the y-axis represents the count. The data appears to be centered around ages 40-60.</li>\n        <li><b>Distribution of Sex:</b> The x-axis represents sex, and the y-axis represents the count. There are more males than females.</li>\n        <li><b>Distribution of Job:</b> The x-axis represents job title, and the y-axis represents the count. There are more people with skilled jobs than unskilled jobs. \"Unskilled and resident\" and \"unskilled and non-resident\" could potentially refer to residency status.</li>\n        <li><b>Distribution of Saving accounts:</b> The x-axis represents the amount of money in saving accounts, and the y-axis represents the count. The data appears to be centered around moderate amounts in saving accounts.</li>\n        <li><b>Distribution of Checking account:</b> The x-axis represents the amount of money in checking accounts, and the y-axis represents the count. The data appears to be centered around moderate amounts in checking accounts.</li>\n        <li><b>Distribution of Housing:</b> The x-axis represents housing type, and the y-axis represents the count. \"Own\" appears to be the most common housing type, followed by \"rent\" and \"moderate\".</li>\n    </ul>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Univariate analysis of categorical data:\nfor i, col in enumerate(cat_cols):\n    fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n\n    # Count of col (countplot)\n    unique_values = df_copy[col].unique()\n    color_list = custom_palette[:len(unique_values)]  # Using custom palette for countplot\n    sns.countplot(data=df_copy, x=col, ax=axes[0], hue=col, palette=color_list)\n    for container in axes[0].containers:\n        axes[0].bar_label(container)\n    axes[0].legend(loc='upper right', bbox_to_anchor=(1.25, 1.02))\n    \n    # Rotate x-axis labels\n    axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n\n    # Count of col (pie chart)\n    slices = df_copy[col].value_counts().sort_index().values\n    activities = [var for var in df_copy[col].value_counts().sort_index().index]\n    axes[1].pie(slices, labels=activities, colors=color_list, shadow=True, autopct='%1.1f%%')\n    axes[1].set_title(f'Pie Chart for {col}')\n\n    plt.suptitle(f'Count of Unique Values in $\\\\mathbf{{{col}}}$ (Fig {i + 1})')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of purchased based on Gender\n%matplotlib inline\n\ndiscrete_cols2 = ['Sex', 'Housing', 'Job', 'Saving accounts', 'Purpose']\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Custom palette for better visualization\n\nfor i, col in enumerate(discrete_cols2):\n    ax = sns.countplot(data=df_copy, x='Risk', hue=col, palette=\"Set2\", ax=axes[i//3, i%3])\n    \n    for container in ax.containers:\n        ax.bar_label(container)\n    \n    # Rotate x-axis labels by 45 degrees\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n\n# Remove the empty subplot\nfig.delaxes(axes[1, 2])\n\nplt.suptitle('Count of Purchased Data Based on Various Categorical Columns', fontsize=16)\nplt.tight_layout(pad=2)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_profile(credit):\n    stats = []\n    for col in credit.columns:\n        stats.append((col, credit[col].nunique(), credit[col].isnull().sum() * 100 / credit.shape[0], credit[col].value_counts(normalize=True, dropna=False).values[0] * 100, credit[col].dtype))\n\n    stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of missing values', 'Percentage of values in the biggest category', 'type'])\n    stats_df.sort_values('Percentage of missing values', ascending=True)\n    return stats_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking unique values\nprint(credit.nunique())\n#Looking the data\nprint(credit.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_profile(credit)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Credit Risk Distribution Report\n\n#### Data Summary\n- **Good Credit:** 700 individuals\n- **Bad Credit:** 300 individuals\n\n#### Visualization Insights\nA grouped bar chart was created to represent the distribution of credit risk:\n\n- **Good Credit:** The bar representing individuals with 'good' credit stands significantly higher, indicating a larger portion of the dataset falls into this category. This suggests a majority of individuals have a positive credit risk profile.\n- **Bad Credit:** The bar representing individuals with 'bad' credit is noticeably shorter, indicating fewer individuals are classified under this category.\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #F9D3CA; padding: 15px; border-radius: 5px;\">\n  <p>Credit cards are issued more frequently to people between the ages of 30 and 50.</p>\n  <p>People with a credit amount of less than \\$5,000  seem to be issued the most credit cards.</p>\n  <p>There seems to be a positive correlation between age and the amount of credit issued. For example, people between 30 and 50 with a credit amount of $5,000 seem to be issued more credit cards compared to people in the same age group but with a higher credit amount.</p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Select numerical columns from the 'credit' DataFrame\nnumerical_credit = credit.select_dtypes(include='number')\n\n# Compute the correlation matrix\ncorrelation_matrix = numerical_credit.corr()\n\n# Plotting the correlation matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"Set2\", fmt=\".2f\", vmin=-1, vmax=1)\nplt.title('Correlation Matrix of Numerical Variables')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"background-color: #F9D3CA; padding: 15px; border-radius: 5px;\">\nSummary\n\nThe correlation matrix in the image shows the correlation between age, job, credit amount, and duration. The values in the table range from -1 to 1. \n\n- The correlation coefficient between age and credit amount is 0.03, which indicates a very weak positive correlation.\n- The correlation coefficient between job and duration is 0.5, which indicates a moderate positive correlation.\n","metadata":{}},{"cell_type":"code","source":"# Suppress specific warnings\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore', message=\"is_categorical_dtype is deprecated\")\n    warnings.filterwarnings('ignore', message=\"The palette list has more values\")\n    warnings.filterwarnings('ignore', message=\"use_inf_as_na option is deprecated\")\n    warnings.filterwarnings('ignore', message=\"When grouping with a length-1 list-like\")\n    warnings.filterwarnings('ignore', message=\"DataFrameGroupBy.apply operated on the grouping columns\")\n\n    # Plotting Age/Sex Distribution using histogram and boxplot\n    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\n    # Histogram of Age distribution colored by Sex using custom palette\n    sns.histplot(credit, x='Age', bins=30, hue=\"Sex\", palette=\"Set2\", ax=ax[0]).set_title(\"Age/Sex Distribution\")\n\n    # Boxplot of Age distribution grouped by Sex using custom palette\n    sns.boxplot(data=credit, x=\"Sex\", y=\"Age\", palette=\"Set2\", ax=ax[1]).set_title(\"Age/Sex Distribution\")\n\n    # Adjust layout and display the figure\n    fig.tight_layout()\n    plt.show()\n\n    # Plotting Credit Amount variation with Age and Duration by Sex using lineplots\n    fig, ax = plt.subplots(2, 1, figsize=(15, 7))\n\n    # Lineplot of Credit Amount by Age colored by Sex using custom palette\n    sns.lineplot(data=credit, x='Age', y='Credit amount', hue='Sex', palette=\"Set2\", lw=2, ax=ax[0]).set_title(\"Credit Amount Graph Depending on Age and Duration by Sex\", fontsize=15)\n\n    # Lineplot of Credit Amount by Duration colored by Sex using custom palette\n    sns.lineplot(data=credit, x='Duration', y='Credit amount', hue='Sex', palette=\"Set2\", lw=2, ax=ax[1])\n\n    # Adjust layout and display the figure\n    fig.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter data for 'good' and 'bad' credit risks and overall age\ndf_good = credit.loc[credit[\"Risk\"] == 'good']['Age'].values.tolist()\ndf_bad = credit.loc[credit[\"Risk\"] == 'bad']['Age'].values.tolist()\ndf_age = credit['Age'].values.tolist()\n\n# Define a custom color palette\ncustom_palette = ['#F6B4A4', '#F78165', '#F46644']\n\n# First plot - Histogram for 'good' credit\ntrace0 = go.Histogram(\n    x=df_good,\n    histnorm='probability',\n    name=\"Good Credit\",\n    marker=dict(color=custom_palette[0])\n)\n\n# Second plot - Histogram for 'bad' credit\ntrace1 = go.Histogram(\n    x=df_bad,\n    histnorm='probability',\n    name=\"Bad Credit\",\n    marker=dict(color=custom_palette[1])\n)\n\n# Third plot - Histogram for overall age distribution\ntrace2 = go.Histogram(\n    x=df_age,\n    histnorm='probability',\n    name=\"Overall Age\",\n    marker=dict(color=custom_palette[2])\n)\n\n# Create subplots with specified grid layout and titles\nfig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                        subplot_titles=('Good Credit', 'Bad Credit', 'General Distribution'))\n\n# Add each trace to the subplot grid\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\n\n# Update layout settings\nfig['layout'].update(showlegend=True, title='Age Distribution', bargap=0.05, height=700, width=1200)\n\n# Plot the figure using offline mode\npy.iplot(fig, filename='custom-sized-subplot-with-subplot-titles')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First plot - Bar chart for sex distribution in 'good' credit\ntrace0 = go.Bar(\n    x=credit[credit[\"Risk\"] == 'good'][\"Sex\"].value_counts().index.values,\n    y=credit[credit[\"Risk\"] == 'good'][\"Sex\"].value_counts().values,\n    name='Good Credit',\n    marker=dict(color=custom_palette[0])\n)\n\n# Second plot - Bar chart for sex distribution in 'bad' credit\ntrace1 = go.Bar(\n    x=credit[credit[\"Risk\"] == 'bad'][\"Sex\"].value_counts().index.values,\n    y=credit[credit[\"Risk\"] == 'bad'][\"Sex\"].value_counts().values,\n    name='Bad Credit',\n    marker=dict(color=custom_palette[1])\n)\n\n# Third plot - Box plot for credit amount distribution by sex in 'good' credit\ntrace2 = go.Box(\n    x=credit[credit[\"Risk\"] == 'good'][\"Sex\"],\n    y=credit[credit[\"Risk\"] == 'good'][\"Credit amount\"],\n    name='Good Credit',\n    marker=dict(color=custom_palette[0])\n)\n\n# Fourth plot - Box plot for credit amount distribution by sex in 'bad' credit\ntrace3 = go.Box(\n    x=credit[credit[\"Risk\"] == 'bad'][\"Sex\"],\n    y=credit[credit[\"Risk\"] == 'bad'][\"Credit amount\"],\n    name='Bad Credit',\n    marker=dict(color=custom_palette[1])\n)\n\n# Combine all traces into a list\ndata = [trace0, trace1, trace2, trace3]\n\n# Create subplots with specified grid layout and titles\nfig = tls.make_subplots(rows=1, cols=2, subplot_titles=('Sex Count', 'Credit Amount by Sex'))\n\n# Add each trace to the subplot grid\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 2)\n\n# Update layout settings for the figure\nfig['layout'].update(height=600, width=1200, title='Sex Distribution and Credit Amount', boxmode='group')\n\n# Plot the figure using offline mode\npy.iplot(fig, filename='sex-subplot')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First plot - Horizontal bar chart for job distribution in 'good' credit\ntrace0 = go.Bar(\n    y=credit[credit[\"Risk\"] == 'good'][\"Job\"].value_counts().index.values,\n    x=credit[credit[\"Risk\"] == 'good'][\"Job\"].value_counts().values,\n    name='Good Credit Distribution',\n    orientation='h',  # Set orientation to horizontal\n    marker=dict(color='#F08080')  # Set color to light coral for 'good' credit\n)\n\n# Second plot - Horizontal bar chart for job distribution in 'bad' credit\ntrace1 = go.Bar(\n    y=credit[credit[\"Risk\"] == 'bad'][\"Job\"].value_counts().index.values,\n    x=credit[credit[\"Risk\"] == 'bad'][\"Job\"].value_counts().values,\n    name='Bad Credit Distribution',\n    orientation='h',  # Set orientation to horizontal\n    marker=dict(color='#FF7F50')  # Set color to coral for 'bad' credit\n)\n\ndata = [trace0, trace1]\n\nlayout = go.Layout(\n    title='Job Distribution',  # Set plot title\n    barmode='group'  # Display bars side by side\n)\n\nfig = go.Figure(data=data, layout=layout)\n\n# Plot the figure\npy.iplot(fig, filename='grouped-bar')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print crosstab summary of 'Purpose' vs 'Risk'\nprint(\"Values describe: \")\nprint(pd.crosstab(credit.Purpose, credit.Risk))\n\n# Set up the plotting environment with a large figure size\nplt.figure(figsize=(14, 12))\n\n# Subplot 1: Countplot of 'Purpose' colored by 'Risk'\nplt.subplot(221)\ng = sns.countplot(x=\"Purpose\", data=credit, \n                  palette={\"good\": \"#F08080\", \"bad\": \"#FF7F50\"}, hue=\"Risk\")\ng.set_xticklabels(g.get_xticklabels(), rotation=45)  # Rotate x-axis labels for better visibility\ng.set_xlabel(\"\", fontsize=12)\ng.set_ylabel(\"Count\", fontsize=12)\ng.set_title(\"Purposes Count\", fontsize=20)\n\n# Subplot 2: Violin plot of 'Age' by 'Purpose' split by 'Risk'\nplt.subplot(222)\ng1 = sns.violinplot(x=\"Purpose\", y=\"Age\", data=credit, \n                    palette={\"good\": \"#F08080\", \"bad\": \"#FF7F50\"}, hue=\"Risk\", split=True)\ng1.set_xticklabels(g1.get_xticklabels(), rotation=45)  # Rotate x-axis labels for better visibility\ng1.set_xlabel(\"\", fontsize=12)\ng1.set_ylabel(\"Age\", fontsize=12)\ng1.set_title(\"Purposes by Age\", fontsize=20)\n\n# Subplot 3: Box plot of 'Credit amount' by 'Purpose' colored by 'Risk'\nplt.subplot(212)\ng2 = sns.boxplot(x=\"Purpose\", y=\"Credit amount\", data=credit, \n                 palette={\"good\": \"#F08080\", \"bad\": \"#FF7F50\"}, hue=\"Risk\")\ng2.set_xlabel(\"Purposes\", fontsize=12)\ng2.set_ylabel(\"Credit Amount\", fontsize=12)\ng2.set_title(\"Credit Amount distribution by Purposes\", fontsize=20)\n\n# Adjust subplot spacing and top margin for better layout\nplt.subplots_adjust(hspace=0.6, top=0.8)\n\n# Display the plot\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Markdown(\"#### Explore the Values of Text Columns:\"))\n# Example DataFrame (replace with your actual data or DataFrame 'df')\ncols = ['Housing', 'Saving accounts', 'Checking account', 'Purpose','Risk']\nfor col in cols:\n    line = \"**\" + col + \":** \"\n    for v in credit[col].unique():\n        line = line + str(v) + \", \"\n    display(Markdown(line))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pd.crosstab(credit[\"Checking account\"],credit.Sex))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\nimport pandas as pd\n\ndef create_donut_pie_chart(credit, feature):\n    # Calculate the ratio and prepare data\n    ratio_size = credit[feature].value_counts(normalize=True) * 100\n    labels = ratio_size.index\n    values = ratio_size.values\n\n    # Define colors (optional)\n    custom_palette = ['blue', 'green', 'orange', 'red', 'purple']  # You can customize the colors\n\n    # Plotly pie chart\n    trace = go.Pie(\n        labels=labels,\n        values=values,\n        hole=0.5,  # Hole creates a donut chart\n        hoverinfo='label+percent+name',\n        marker=dict(colors=custom_palette),\n    )\n\n    # Layout\n    layout = go.Layout(\n        title='Checking Account vs Sex Distribution',\n        annotations=[\n            {'text': 'Male', 'x': 0.2, 'y': 0.5, 'showarrow': False},\n            {'text': 'Female', 'x': 0.8, 'y': 0.5, 'showarrow': False}\n        ]\n    )\n\n    # Create Plotly figure\n    fig = go.Figure(data=[trace], layout=layout)\n\n    # Show the figure\n    fig.show()\n\n# Example usage with your data\n# Assuming 'credit' is your DataFrame with 'Checking account' and 'Sex' columns\ncreate_donut_pie_chart(credit, 'Checking account')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crosstabulation of 'Purpose' vs 'Sex'\ncross_tab = pd.crosstab(credit[\"Purpose\"], credit[\"Sex\"])\n\n# Plotting the grouped bar chart with specified colors\n\n# Plotting\ncross_tab.plot(kind='bar', figsize=(10, 6), rot=45, color=custom_palette)\n\n# Adding labels and title\nplt.title('Cross-tabulation of Purpose vs Sex')\nplt.xlabel('Purpose')\nplt.ylabel('Count')\nplt.xticks(range(len(cross_tab.index)), cross_tab.index)  # Set x-axis ticks to match Purpose categories\nplt.legend(title='Sex')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label encode account quality and fill NaN with 0\ndef SC_LabelEncoder(text):\n    if text == \"little\":\n        return 1\n    elif text == \"moderate\":\n        return 2\n    elif text == \"quite rich\":\n        return 3\n    elif text == \"rich\":\n        return 4\n    else:\n        return 0\n\ncredit[\"Saving accounts\"] = credit[\"Saving accounts\"].apply(SC_LabelEncoder)\ncredit[\"Checking account\"] = credit[\"Checking account\"].apply(SC_LabelEncoder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label encode account quality and fill NaN with 0\ndef H_LabelEncoder(text):\n    if text == \"free\":\n        return 0\n    elif text == \"rent\":\n        return 1\n    elif text == \"own\":\n        return 2\n\ncredit[\"Housing\"] = credit[\"Housing\"].apply(H_LabelEncoder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Pre-processing For Discrete Categorical Columns","metadata":{}},{"cell_type":"code","source":"# use LabelEncoder() to encode other categorical columns:\nfrom sklearn.preprocessing import LabelEncoder\nfor col in [\"Sex\", \"Purpose\", \"Age\"]:\n    le = LabelEncoder()\n    le.fit(credit[col])\n    credit[col] = le.transform(credit[col])\ncredit.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#F2A997;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <h3 style=\"padding: 8px;color:white\"><b>4 | Model  </b></h3>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#F2A997;font-size:100%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <h3 style=\"padding: 8px;color:white\"><b>4.1 | Kmean clustring  </b></h3>","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\", message=\"KMeans is known to have a memory leak on Windows with MKL\")\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"The default value of `n_init` will change\")\n\n\n# Standardize the numerical features of the credit dataset\nscaler = StandardScaler()\nscaled_credit = scaler.fit_transform(numerical_credit)\n\n# Initialize an empty list to store inertia values for different k values\ndistance = []\n\n# Iterate over a range of k values (number of clusters)\nfor k in range(1, 10):\n    # Initialize and fit the KMeans model\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(scaled_credit)\n    \n    # Append the inertia (sum of squared distances to the nearest cluster center) to the list\n    distance.append(kmeans.inertia_)\n\n# Plot the inertia values for each k value to visualize the Elbow Method\nplt.plot(range(1, 10), distance, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Inertia')\nplt.title('The Elbow Method')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a 4x2 grid of subplots with a specified figure size\nfig, ax = plt.subplots(4, 2, figsize=(24, 20))\n\n# Loop through cluster numbers from 2 to 9\nfor i in range(2, 10):\n    # Create a KMeans instance with i clusters\n    km = KMeans(n_clusters=i)\n    \n    # Determine the position of the subplot\n    # q: quotient for row index, r: remainder for column index\n    q, r = divmod(i, 2)\n    \n    # Create a SilhouetteVisualizer instance for the KMeans model\n    # Associate it with the appropriate subplot axis\n    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q - 1][r])\n    \n    # Fit the visualizer on the scaled credit data\n    visualizer.fit(scaled_credit)\n    \n    # Set the title for the subplot\n    ax[q - 1][r].set_title(\"For Cluster_\" + str(i))\n    \n    # Set the x-axis label for the subplot\n    ax[q - 1][r].set_xlabel(\"Silhouette Score\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a KMeans model instance\nmodel = KMeans()\n\n# Instantiate the KElbowVisualizer with the model, specifying the range of k values (2 to 10)\n# Also, specify the metric to evaluate the number of clusters (Calinski-Harabasz criterion)\n# Turn off timings to avoid displaying the fit times\nvisualizer = KElbowVisualizer(model, k=(2, 10), metric='calinski_harabasz', timings=False)\n\n# Fit the visualizer on the scaled credit data\nvisualizer.fit(scaled_credit)\n\n# Display the visualization\nvisualizer.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize OptimalK with specified parameters\noptimalK = OptimalK(n_jobs=8, parallel_backend='joblib')\n\n# Compute optimal number of clusters using Gap Statistics\nn_clusters = optimalK(scaled_credit, cluster_array=np.arange(1, 10))\n\n# Retrieve Gap Statistics result dataframe\ngap_result = optimalK.gap_df\n\n# Plotting Gap Statistics results\nplt.plot(gap_result.n_clusters, gap_result.gap_value)\n\n# Customize plot with dashed red line indicating maximum Gap value\nplt.axhline(np.max(gap_result.gap_value), color='r', linestyle='dashed', linewidth=2)\n\n# Set plot title and axis labels\nplt.title('Gap Analysis')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Gap Value')\n\n# Display the plot\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize KMeans model with n clusters\nkmeans = KMeans(n_clusters=3)\n\n# Fit the KMeans model and predict clusters\nclusters = kmeans.fit_predict(scaled_credit)\n\n# Create a figure with subplots\nplt.figure(figsize=(10, 12))\n\n# Subplot 1: Age vs Credit\nplt.subplot(311)\nplt.scatter(scaled_credit[:, 0], scaled_credit[:, 2], c=kmeans.labels_, cmap=\"Set2\")\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 2], s=80, marker='x', color='k')\nplt.title('Age vs Credit')\n\n# Subplot 2: Credit vs Duration\nplt.subplot(312)\nplt.scatter(scaled_credit[:, 0], scaled_credit[:, 2], c=kmeans.labels_, cmap=\"Set2\")\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 2], s=80, marker='x', color='k')\nplt.title('Credit vs Duration')\n\n# Subplot 3: Age vs Duration\nplt.subplot(313)\nplt.scatter(scaled_credit[:, 2], scaled_credit[:, 3], c=kmeans.labels_, cmap=\"Set2\")\nplt.scatter(kmeans.cluster_centers_[:, 2], kmeans.cluster_centers_[:, 3], s=120, marker='x', color='k')\nplt.title('Age vs Duration')\n\n# Adjust layout and display the plot\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spliting X and y into train and test version\nX, y = credit.drop(\"Risk\", axis=1), credit[\"Risk\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"credit = pd.read_csv('/kaggle/input/german-credit-data-with-risk/german_credit_data.csv')\ndel credit['Unnamed: 0']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define mappings for categorical columns\nsex_mapping = {'male': 1, 'female': 2}\nhousing_mapping = {'own': 1, 'rent': 2, 'free': 3}\nsavings_mapping = {'little': 1, 'moderate': 2, 'quite rich': 3, 'rich': 4}\nchecking_mapping = {'little': 1, 'moderate': 2, 'rich': 3}\npurpose_mapping = {\n    'radio/TV': 1, 'furniture/equipment': 2, 'education': 3, 'car': 4,\n    'business': 5, 'maintenance': 6, 'travel': 7, 'other purposes': 8\n}\nrisk_mapping = {'bad': 0, 'good': 1}\n\n# Apply mappings only to existing columns\nif 'Sex' in credit.columns:\n    credit['Sex'] = credit['Sex'].map(sex_mapping)\n\nif 'Housing' in credit.columns:\n    credit['Housing'] = credit['Housing'].map(housing_mapping)\n\nif 'Saving accounts' in credit.columns:\n    credit['Saving accounts'] = credit['Saving accounts'].map(savings_mapping)\n\nif 'Checking account' in credit.columns:\n    credit['Checking account'] = credit['Checking account'].map(checking_mapping)\n\nif 'Purpose' in credit.columns:\n    credit['Purpose'] = credit['Purpose'].map(purpose_mapping)\n\nif 'Risk' in credit.columns:\n    credit['Risk'] = credit['Risk'].map(risk_mapping)\n\n# Optionally, convert any remaining non-numeric columns to numeric if needed\ncredit = credit.apply(pd.to_numeric, errors='ignore')\ncredit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"credit.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"credit.fillna(0, inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming 'credit' is your original DataFrame with features including 'Sex' and target 'Risk'\n\n# Ignore all warnings\nwarnings.filterwarnings('ignore')\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ncredit['Sex'] = label_encoder.fit_transform(credit['Sex'])\n\n# Split the data into X (features) and y (target)\nX = credit.drop(['Risk'], axis=1)\ny = credit['Risk']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n\n# Initialize variables to track maximum score and corresponding k value\nmax_score = 0\nmax_k = 0\n\n# Loop through k values from 1 to 99 (inclusive)\nfor k in range(1, 25):\n    # Create a KNeighborsClassifier model with current k value\n    neigh = KNeighborsClassifier(n_neighbors=k)\n    \n    # Fit the model on training data\n    neigh.fit(X_train, y_train)\n    \n    # Compute F1 score on test data\n    score = f1_score(y_test, neigh.predict(X_test), average='weighted')  # Use weighted average for multi-class classification\n    \n    # Check if current score is greater than maximum score found so far\n    if score > max_score:\n        max_k = k  # Update the k value for maximum score\n        max_score = score  # Update the maximum score\n\n# Display the results using Markdown syntax\ndisplay(Markdown(\"If using K-Nearest Neighbors Classification, the optimal k should be \" + str(max_k) + \n                 \" to achieve the best prediction. The weighted average F1 score at this k value is \" + str(max_score)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn_model = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn_model.fit(X_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn_model.score(X_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn_model.score(X_test, y_test))\n\n# Plot\nplt.figure(figsize=[12,6])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('Value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#F2A997;font-size:100%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <h3 style=\"padding: 8px;color:white\"><b>4.2 | Probability of Default Estimation with Logistic Regression  </b></h3>","metadata":{}},{"cell_type":"code","source":"clusters, counts = np.unique(kmeans.labels_, return_counts=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_dict = {}\nfor i in range(len(clusters)):\n    cluster_dict[i] = scaled_credit[np.where(kmeans.labels_==i)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"credit['clusters'] = pd.DataFrame(kmeans.labels_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scaled = pd.DataFrame(scaled_credit)\ndf_scaled['clusters'] = credit['clusters']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scaled['Risk'] = credit['Risk']\ndf_scaled.columns = ['Age', 'Job', 'Credit amount',\n'Duration', 'Clusters', 'Risk']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scaled[df_scaled.Clusters == 0]['Risk'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scaled[df_scaled.Clusters == 1]['Risk'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scaled[df_scaled.Clusters == 0]['Risk'].value_counts().plot(kind='bar',figsize=(10, 6),title=\"Frequency of Risk Level\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scaled[df_scaled.Clusters == 1]['Risk'].value_counts().plot(kind='bar',figsize=(10, 6),title=\"Frequency of Risk Level\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scaled['Risk'] = df_scaled['Risk'].replace({'good': 1, 'bad': 0})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_scaled.drop('Risk', axis=1)\ny = df_scaled.loc[:, ['Risk', 'Clusters']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_cluster_train = X_train[X_train.Clusters == 0].iloc[:, :-1]\nsecond_cluster_train = X_train[X_train.Clusters == 1].iloc[:, :-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train1 = first_cluster_train\ny_train1 = y_train[y_train.Clusters == 0]['Risk']\nsmote = SMOTEENN(random_state = 2)\nX_train1, y_train1 = smote.fit_resample(X_train1, y_train1.ravel())\nlogit = sm.Logit(y_train1, X_train1)\nlogit_fit1 = logit.fit()\nprint(logit_fit1.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_cluster_test = X_test[X_test.Clusters == 0].iloc[:, :-1]\nsecond_cluster_test = X_test[X_test.Clusters == 1].iloc[:, :-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test1 = first_cluster_test\ny_test1 = y_test[y_test.Clusters == 0]['Risk']\npred_prob1 = logit_fit1.predict(X_test1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"false_pos, true_pos, _ = roc_curve(y_test1.values, pred_prob1)\nauc = roc_auc_score(y_test1, pred_prob1)\nplt.plot(false_pos,true_pos, label=\"AUC for cluster 1={:.4f} \".format(auc))\nplt.plot([0, 1], [0, 1], linestyle = '--', label='45 degree line')\nplt.legend(loc='best')\nplt.title('ROC-AUC Curve 1')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train2 = second_cluster_train\ny_train2 = y_train[y_train.Clusters == 1]['Risk']\nlogit = sm.Logit(y_train2, X_train2)\nlogit_fit2 = logit.fit()\nprint(logit_fit2.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test2 = second_cluster_test\ny_test2 = y_test[y_test.Clusters == 1]['Risk']\npred_prob2 = logit_fit2.predict(X_test2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"false_pos, true_pos, _ = roc_curve(y_test2.values, pred_prob2)\nauc = roc_auc_score(y_test2, pred_prob2)\nplt.plot(false_pos,true_pos,label=\"AUC for cluster 2={:.4f} \"\n.format(auc))\nplt.plot([0, 1], [0, 1], linestyle = '--', label='45 degree line')\nplt.legend(loc='best')\nplt.title('ROC-AUC Curve 2')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#F2A997;font-size:100%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <h3 style=\"padding: 8px;color:white\"><b>4.3 | Probability of Default Estimation with SVC  ,Decision Tree Classifier   ,RandomForestClassifier   ,GaussianNB  </b></h3>","metadata":{}},{"cell_type":"code","source":"# define models\nModels = {\n    \"SVC\": SVC(),\n    \"DecisionTree\": DecisionTreeClassifier(),\n    \"RandomForest\": RandomForestClassifier(),\n    \"GaussianNaiveBayes\": GaussianNB(),\n   \n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"credit.drop('clusters', axis=1, inplace=True)\ncredit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spliting X and y into train and test version\nX, y = credit.drop(\"Risk\", axis=1), credit[\"Risk\"]\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.20, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming 'credit' is your original DataFrame with features including 'Sex' and target 'Risk'\n# and 'Models' is a dictionary of your models\n\n# Create an empty list to store results\nresults = []\n\n# Loop through each model in the Models dictionary\nfor key in Models.keys():\n    try:\n        # Perform cross-validation for the current model\n        cv_res = cross_validate(Models[key], X_train, y_train.ravel(),  # Ensure y_train is 1D\n                                return_train_score=True,\n                                scoring=\"f1\",\n                                cv=5, n_jobs=-1,\n                                error_score='raise')\n\n        # Calculate mean scores and times from cross-validation results\n        res = {\n            'model': key,\n            'train_score': cv_res[\"train_score\"].mean(),\n            'test_score': cv_res[\"test_score\"].mean(),\n            'fit_time': cv_res[\"fit_time\"].mean(),\n            'score_time': cv_res[\"score_time\"].mean()\n        }\n\n        # Append the results dictionary to the list\n        results.append(res)\n\n        # Print a message indicating completion of cross-validation for the current model\n        print(\"CV for model:\", key, \"done.\")\n\n    except Exception as e:\n        # Print the exception message if an error occurs\n        print(f\"CV for model {key} failed with error: {str(e)}\")\n\n# Create a DataFrame from the results list\ncv_results = pd.DataFrame(results)\n\n# Display the results using Markdown syntax\ndisplay(Markdown(\"### Cross-validation Results\"))\ndisplay(cv_results.style.background_gradient(cmap=\"Set2\"))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the results with x-axis rotation of 45 degrees\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 15))\n\n# Bar plot for train and test scores\naxes[0].bar(cv_results['model'], cv_results['train_score'], color='skyblue', label='Train Score')\naxes[0].bar(cv_results['model'], cv_results['test_score'], color='orange', label='Test Score', alpha=0.7)\naxes[0].set_title('Train and Test Scores')\naxes[0].set_xlabel('Models')\naxes[0].set_ylabel('F1 Score')\naxes[0].legend()\naxes[0].tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n\n# Bar plot for fit time\naxes[1].bar(cv_results['model'], cv_results['fit_time'], color='lightgreen')\naxes[1].set_title('Fit Time')\naxes[1].set_xlabel('Models')\naxes[1].set_ylabel('Time (seconds)')\naxes[1].tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n\n# Bar plot for score time\naxes[2].bar(cv_results['model'], cv_results['score_time'], color='lightcoral')\naxes[2].set_title('Score Time')\naxes[2].set_xlabel('Models')\naxes[2].set_ylabel('Time (seconds)')\naxes[2].tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n\n# Adjust layout and show the plots\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the RandomForest model using training data\nrf = Models[\"RandomForest\"].fit(X_train, y_train)\n\n# Compute F1 score on test data using the trained RandomForest model\nprint('f1_score:', f1_score(y_test, rf.predict(X_test)))\n\n# Importing Support Vector Classifier from sklearn\nfrom sklearn.svm import SVC\n\n# Creating an empty DataFrame to store feature importances\nfeature_importance = pd.DataFrame()\n\n# Assigning column names to the empty DataFrame\nfeature_importance[\"feature\"] = X_train.columns\n\n# Assigning feature importance scores to the 'importance' column\nfeature_importance[\"importance\"] = rf.feature_importances_\n\n# Sorting feature importances in descending order\nfeature_importance = feature_importance.sort_values(\"importance\", ascending=False)\n\n# Displaying the sorted feature importances\nfeature_importance\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a hypothetical example for prediction\nexample = pd.DataFrame({\n    'Age': [20],\n    'Sex': [1],  # 1 for male as per your mapping\n    'Job': [1],\n    'Housing': [1],  # 1 for own as per your mapping\n    'Saving accounts': [2],  # 2 for moderate as per your mapping\n    'Checking account': [1],  # 2 for moderate as per your mapping\n    'Credit amount': [500],\n    'Duration': [3],\n    'Purpose': [2]  # 2 for furniture/equipment as per your mapping\n})\n\n# Assuming 'Models' is your dictionary of models and 'RandomForest' is your Random Forest model\n# Select and fit the Random Forest model\nrf_model = Models[\"RandomForest\"]\nrf_model.fit(X_train, y_train)\n\n# Predict probability of default for the example\nprobability_of_default = rf_model.predict_proba(example)\n\n# Extract the probability of the positive class (default) which is index 1\nprobability_of_default = probability_of_default[:, 1]\n\n# Print the probability of default\nprint(\"Probability of default:\", probability_of_default)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Customer Credit Risk Assessment with model of Random forest\n\n- **Age:** 20\n- **Sex:** Male\n- **Job:** Unskilled and resident\n- **Housing:** Own\n- **Saving accounts:** Moderate\n- **Checking account:** Moderate\n- **Credit amount:** 500\n- **Duration:** 3 years\n- **Purpose:** Furnishings/equipment\n\n### Probability of Default: 45%\n\nBased on these attributes, the estimated probability of default is 48%. This assessment considers factors such as age, gender, job status, housing type, savings and checking account status, credit amount, loan duration, and loan purpose.\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#F2A997;font-size:100%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <h3 style=\"padding: 8px;color:white\"><b>4.3 | Probability of Default Estimation with Neural Network  </b></h3>","metadata":{}},{"cell_type":"code","source":"# Define hyperparameters grid for MLPClassifier\nparam_NN = {\n    \"hidden_layer_sizes\": [(100, 50), (50, 50), (10, 100)],\n    \"solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n    \"learning_rate_init\": [0.001, 0.05]\n}\n\n# Initialize MLPClassifier with default settings\nMLP = MLPClassifier(random_state=42)\n\n# Perform HalvingRandomSearchCV for first cluster\nparam_halve_NN = HalvingRandomSearchCV(MLP, param_NN, scoring='roc_auc')\nparam_halve_NN.fit(X_train, y_train)\n\n# Print best hyperparameters and ROC AUC score for first cluster\nprint('Best hyperparameters for first cluster in MLPClassifier are:')\nprint(param_halve_NN.best_params_)\ny_pred_NN1 = param_halve_NN.predict(X_test)\nprint('ROC AUC score of MLPClassifier for first cluster: {:.4f}'.format(roc_auc_score(y_test, y_pred_NN1)))\n\n# Perform HalvingRandomSearchCV for second cluster\nparam_halve_NN.fit(X_train, y_train)\n\n# Print best hyperparameters and ROC AUC score for second cluster\nprint('\\nBest hyperparameters for second cluster in MLPClassifier are:')\nprint(param_halve_NN.best_params_)\ny_pred_NN2 = param_halve_NN.predict(X_test)\nprint('ROC AUC score of MLPClassifier for second cluster: {:.4f}'.format(roc_auc_score(y_test, y_pred_NN2)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example new data\nnew_data = pd.DataFrame({\n    'Age': [20],\n    'Sex': [1],  # 1 for male as per your mapping\n    'Job': [1],\n    'Housing': [1],  # 1 for own as per your mapping\n    'Saving accounts': [2],  # 2 for moderate as per your mapping\n    'Checking account': [1],  # 1 for moderate as per your mapping\n    'Credit amount': [500],\n    'Duration': [3],\n    'Purpose': [2]  # 2 for furniture/equipment as per your mapping\n})\n# Predict probabilities for the first cluster\nprobabilities_cluster1 = param_halve_NN.predict_proba(new_data)\n\n# Assuming you have separate models for each cluster\n# Predict probabilities for the second cluster\nprobabilities_cluster2 = param_halve_NN.predict_proba(new_data)\n# Extracting probabilities of default (class 1)\nprob_default_cluster1 = probabilities_cluster1[:, 1]\nprob_default_cluster2 = probabilities_cluster2[:, 1]\n\n# Printing the probabilities\nprint('Probability of default for the first cluster:', prob_default_cluster1)\nprint('Probability of default for the second cluster:', prob_default_cluster2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Credit Risk Assessment Report\n\n### Customer Profile\n- **Age:** 20\n- **Sex:** Male\n- **Job:** Unskilled and resident\n- **Housing:** Own\n- **Saving accounts:** Moderate\n- **Checking account:** Moderate\n- **Credit amount:** 500\n- **Duration:** 3 years\n- **Purpose:** Furnishings/equipment\n\n### Risk Assessment\nBased on the provided customer profile, the following probabilities of default have been estimated:\n\n- **Probability of default for the first cluster:** 63.75%\n- **Probability of default for the second cluster:** 63.75%\n\n### Analysis and Recommendations\n\nThe estimated probabilities of default for both clusters are identical, indicating a consistent risk assessment regardless of cluster assignment. This suggests that the customer's profile poses a moderately high risk of defaulting on credit obligations. Factors contributing to this risk include the customer's young age, relatively low credit amount relative to duration, and possibly other undisclosed financial liabilities.\n\n#### Recommendations:\n1. **Monitor Closely:** Given the moderate risk profile, closely monitor the customer's repayment behavior during the initial period.\n   \n2. **Consider Mitigation Strategies:** Implement strategies such as shorter loan durations or stricter repayment terms to mitigate potential default risks.\n\n3. **Further Assessment:** Conduct additional assessments, including credit score analysis or income verification, to refine the risk assessment.\n\nThis report provides a preliminary risk assessment based on the provided data. For more accurate and tailored recommendations, further detailed financial information and credit history would be beneficial.\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#F2A997;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <h3 style=\"padding: 8px;color:white\"><b>5 | References </b></h3>","metadata":{}},{"cell_type":"markdown","source":"1. “RandomForestClassifier.” Sklearn, scikit-learn-ts-git-feature-docs-2-saasify.vercel.app/docs/classes/RandomForestClassifier. Accessed 22 June 2and\n\n2. “Sklearn.tree.DecisionTreeClassifier — Scikit-Learn 0.22.1 Documentation.” Scikit-Learn.org, 2019, scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html.\n\n3. Chern, Ching-Chin, et al. “A Decision Tree Classifier for Credit Assessment Problems in Big Data Environments.” Information Systems and E-Business Management, vol. 19, no. 1, 2 Feb. 2021, pp. 363–386, https://doi.org/10.1007/s10257-021-00511-w.\n\n4. Journal, Engineering And Technology. “Analysis of Credit Card Fraud Detection Performance Using Random Forest Classifier & Neural Networks Model | Engineering and Technology Journal.” Www.everant.org, www.everant.org/index.php/etj/article/view/1196. Accessed 22 June 2024.\n\n5. Shi, Lei, et al. “Credit Assessment with Random Forests.” Springer EBooks, 1 Jan. 2011, pp. 24–28, https://doi.org/10.1007/978-3-642-24282-3_4. Accessed 15 Jan. 2024.\n\n6. S, LAVANYA. “Gaussian Naive Bayes Algorithm for Credit Risk Modelling.” Analytics Vidhya, 1 Mar. 2022, www.analyticsvidhya.com/blog/2022/03/gaussian-naive-bayes-algorithm-for-credit-risk-modelling/.\n\n7. “Sklearn.naive_bayes.GaussianNB — Scikit-Learn 0.22.1 Documentation.” Scikit-Learn.org, scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html.\n\n8. “Machine Learning for Financial Risk Management with Python [Book].” Www.oreilly.com, www.oreilly.com/library/view/machine-learning-for/9781492085249/. Accessed 22 June 2024.","metadata":{}}]}